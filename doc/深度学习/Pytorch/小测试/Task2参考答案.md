

##Task2：
Q1: 我们常用Adam和SGD两种优化器，不过这两种优化器该如何选择，并且二者的优劣在哪里？

A1: 建议看一下吴恩达网易云课堂关于Adam的部分，https://mooc.study.163.com/learn/2001281003?tid=2001391036&_trace_c_p_k2_=7f8ddb27ae2449af854a44c050993fbf#/learn/content?type=detail&id=2001701052

Q2: 计算图是用来做什么的？pytorch 动态计算图与 tensorflow的静态计算图有什么区别么？

A2: 在pytorch中计算图用来记录变量的操作，在反向传播时根据计算图自动计算梯度，动态计算图的好处是一个tensor在进行了一步操作后就可以看到相关变量的变化，可以输出中间变量，而静态计算图只有在最后执行计算时，才会根据整个计算图执行操作。

Q3: pytorch的nn.Sequential建立神经网络的方式和一般定义在__init__里面的方式是一样的吗？这两种有高效之分吗？

A3: 我觉得没有高效之分，不过在使用class构建自己的神经网络的好处是可以在类中定义很多自定义方法，变量，而使用nn.Sequential只能使用pytorch定义好的一些层结构，不灵活。

Q4: 在训练神经网络时，应该如何选取优化器，我看例子大多是用SGD,但是pytorch里有很多其他的优化器实现，这个选取有什么技巧吗？还有，torch.optim.lr_scheduler类中的方法可以用来调整学习率，我们应该如何使用它们训练出准确率高，同时避免过拟合的模型呢？

A4: 选择优化器可以看q2的回答。使用torch.optim.lr_scheduler一般是让学习率衰减避免学习率过大，不易收敛到全局最优。

Q5: 为什么在搭建神经网络的时候好多都使用了relu函数？例如cnn中，使用relu的意义是什么？什么时候需要使用relu激活函数？

A5: 第一，采用sigmoid等函数，反向传播求误差梯度时，求导计算量很大，而Relu求导非常容易。
第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0），从而无法完成深层网络的训练。
第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。