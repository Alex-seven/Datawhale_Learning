# 目录
**Task1**
Q1: 我们在选取batch_size的时候，如果样本数除以batch_size还有剩余，也就是说剩余的数量不足一个batch_size，此时pytorch会怎么处理?
Q2: net(input)这里是重载了（）吗？为啥可以输入参数呢？
Q3: pytorch文档中介绍：“Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。”还是不太理解什么是张量？putorch为什么要引入张量？为什么可以在GPU上加速计算？
Q4: pytorch相关深度学习算法的理论部分需要掌握多少，理论部分的掌握程度在实际应用中的影响有多大
Q5: 误差反向传播是怎么样计算出参数更新值的？

**Task2**
Q1: 我们常用Adam和SGD两种优化器，不过这两种优化器该如何选择，并且二者的优劣在哪里？
Q2: 计算图是用来做什么的？pytorch 动态计算图与 tensorflow的静态计算图有什么区别么？
Q3: pytorch的nn.Sequential建立神经网络的方式和一般定义在__init__里面的方式是一样的吗？这两种有高效之分吗？
Q4: 在训练神经网络时，应该如何选取优化器，我看例子大多是用SGD,但是pytorch里有很多其他的优化器实现，这个选取有什么技巧吗？还有，torch.optim.lr_scheduler类中的方法可以用来调整学习率，我们应该如何使用它们训练出准确率高，同时避免过拟合的模型呢？
Q5: 为什么在搭建神经网络的时候好多都使用了relu函数？例如cnn中，使用relu的意义是什么？什么时候需要使用relu激活函数？

**Task3**
Q1: 数据的归一化是否是必须的，不进行归一化操作的影响是什么
Q2: 线性回归和逻辑回归有什么区别？
Q3: 不太明白，对于多分类的任务，逻辑回归出的结果是什么？
Q4: 为什么在数据集比较小的时候SGD的表现不尽人意？有没有什么优化方案
Q5: 通常的话感觉nn.Sequential可以更快捷的搭建，那么基于nn.module搭建的优势在什么地方呢？

**Task4**
Q1: 回归问题和分类问题在优化器选择上有没有需要注意的地方？
Q2: 如何根据具体任务选择要优化的loss function？
Q3: 优化网络的常用方法有哪些
Q4: 网络的层数和模型效果是成比例吗？
Q5: 为什么卷积层在图像识别中会有很好的效果？

**Task5**
Q1: dropout的概率是否依赖于网络结构，例如卷积层，全连接层的dropout概率是否有推荐的取值
Q2: ：L1、L2和Dropout的异同是什么？防止过拟合还有什么其他的方法呢
Q3: 权重衰减和丢弃法哪种更常用
Q4: 既然算法很多都可以使用numpy和pytorch进行实现，是不是理论上说numpy可以替代pytorch?
Q5: 在训练阶段和测试阶段，dropout有何不同？BN层在训练和测试阶段有什么不同，均值和方差是如何计算出来的？

**Task6**
Q1: 优化器对于不同的场景选择上有没有要求？或者说在使用时必须把这几个进行对比？然后取出最优？
Q2: 通常来说优化器的参数很重要，那么我们通过什么样的方法可以确定最优参数呢？
Q3: 一定是Adam优化器最好用吗？或者说有没有SGD优化强于Adam的情况？
Q4: 随机梯度下降的特点是什么？怎么实现
Q5: Adam和RMSprop是否适用于大部分神经网络的训练？

**Task7**
Q1: 网络初始化部分在哪里体现，每次搭建网络都需要重新写网络初始化部分吗？如何写网络初始化部分代码？
Q2: 想问问写了一个基本的，可以从哪些方面来考虑优化，怎么对正确率进行提升？
Q3: 神经网络解决过拟合的方法有哪些，如何选择？
Q4: 数据集进行正态化处理时，均值0.1307和方差0.3081怎么得来的，会根据数据集的不同而改变吗？
Q5: 以后进一步学习的资源去哪找，有推荐的吗？

# Task1：
**Q1: 我们在选取batch_size的时候，如果样本数除以batch_size还有剩余，也就是说剩余的数量不足一个batch_size，此时pytorch会怎么处理?**

A1: 其实batchsize只是批量输入数据而已，就好比批量计算
1+2
3+5
4+6
然后突然最后一批只剩下
2+3
这个对于计算没有任何影响。直接把剩下的数据计算完就可以

**Q2: net(input)这里是重载了（）吗？为啥可以输入参数呢？**

A2: 是因为net继承了nn.Module,重载了__call__方法，在__call__方法中默认调用了forward（)方法,  net(input)默认是调用了net.forward(input)方法。

**Q3: pytorch文档中介绍：“Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。”还是不太理解什么是张量？putorch为什么要引入张量？为什么可以在GPU上加速计算？**

A3: 张量它和ndarray的一个区别在于它相当于源代码，告诉gpu怎么计算。
它就好比告诉电脑 x+y 这个公式，然后具体的值数据量非常大一次性给清方便批量计算。而ndarray它就好比告诉电脑
1+2
5+6
7+8
这些数据要计算。
用张量有两个好处一来它可以用来自动求导（直接告诉电脑1+2是没法求导的），而来可以gpu根据它进行批量计算。

**Q4: pytorch相关深度学习算法的理论部分需要掌握多少，理论部分的掌握程度在实际应用中的影响有多大**

A4: 用pytorch搭建深度学习暂时只用了解神经网络的输入格式和输出矩阵形状怎么告诉pytorch。怎么告诉pytorch每层神经网络输入矩阵和输出矩阵的形状。
然后要pytorch怎么向神经网络输入数据，和标签。
要了解训练到底在做什么，训练好了怎么进行预测。
如果是卷积神经网络。那就要了解卷积神经网络中的卷积操作，池化操作怎么做的。怎么在pytorch实现。
 关于如果只想快速学习pytorch就不要纠结梯度下降和反向传播的细节。只用关注怎么设计网络怎么输入数据。

**Q5: 误差反向传播是怎么样计算出参数更新值的？**

A5: 反向传播就梯度下降的求导部分，它做的工作就是梯度下降求导有些重复计算的结果就直接保存。
所以了解参数更新只需要了解梯度下降就可以。
关于梯度下降可以看这篇文章
[https://zhuanlan.zhihu.com/p/43452377](https://zhuanlan.zhihu.com/p/43452377)
# Task2：
**Q1: 我们常用Adam和SGD两种优化器，不过这两种优化器该如何选择，并且二者的优劣在哪里？**

A1: 建议看一下吴恩达网易云课堂关于Adam的部分，[https://mooc.study.163.com/learn/2001281003?tid=2001391036&_trace_c_p_k2_=7f8ddb27ae2449af854a44c050993fbf#/learn/content?type=detail&id=2001701052](https://mooc.study.163.com/learn/2001281003?tid=2001391036&_trace_c_p_k2_=7f8ddb27ae2449af854a44c050993fbf#/learn/content?type=detail&id=2001701052)

**Q2: 计算图是用来做什么的？pytorch 动态计算图与 tensorflow的静态计算图有什么区别么？**

A2: 在pytorch中计算图用来记录变量的操作，在反向传播时根据计算图自动计算梯度，动态计算图的好处是一个tensor在进行了一步操作后就可以看到相关变量的变化，可以输出中间变量，而静态计算图只有在最后执行计算时，才会根据整个计算图执行操作。

**Q3: pytorch的nn.Sequential建立神经网络的方式和一般定义在__init__里面的方式是一样的吗？这两种有高效之分吗？**

A3: 我觉得没有高效之分，不过在使用class构建自己的神经网络的好处是可以在类中定义很多自定义方法，变量，而使用nn.Sequential只能使用pytorch定义好的一些层结构，不灵活。

**Q4: 在训练神经网络时，应该如何选取优化器，我看例子大多是用SGD,但是pytorch里有很多其他的优化器实现，这个选取有什么技巧吗？还有，torch.optim.lr_scheduler类中的方法可以用来调整学习率，我们应该如何使用它们训练出准确率高，同时避免过拟合的模型呢？**

A4: 选择优化器可以看q2的回答。使用torch.optim.lr_scheduler一般是让学习率衰减避免学习率过大，不易收敛到全局最优。

**Q5: 为什么在搭建神经网络的时候好多都使用了relu函数？例如cnn中，使用relu的意义是什么？什么时候需要使用relu激活函数？**

A5: 第一，采用sigmoid等函数，反向传播求误差梯度时，求导计算量很大，而Relu求导非常容易。
第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0），从而无法完成深层网络的训练。
第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。
# Task3：
**Q1: 数据的归一化是否是必须的，不进行归一化操作的影响是什么**

A1: 数据归一化是为了使数据处于同一个尺度下。不进行数据归一化的话，在进行梯度下降的时候会需要更多的迭代次数才能收敛。

**Q2: 线性回归和逻辑回归有什么区别？**

A2: 线性回归就是wx+b, 逻辑回归就是sigmoid(wx+b)，其实逻辑回归就是把在线性回归上加了一层sigmoid函数，让本来只能解决回归问题的线性回归可以用来解决分类问题。

**Q3: 不太明白，对于多分类的任务，逻辑回归出的结果是什么？**

A3: 一般是one VS other的概率，就是得到样本属于某一个分类和其余分类的概率。

**Q4: 为什么在数据集比较小的时候SGD的表现不尽人意？有没有什么优化方案**
A4: 数据增强或者改用其他比较稳定的优化器例如Adam

**Q5: 通常的话感觉nn.Sequential可以更快捷的搭建，那么基于nn.module搭建的优势在什么地方呢？**

A5: 我觉得没有高效之分，不过在使用class构建自己的神经网络的好处是可以在类中定义很多自定义方法，变量，而使用nn.Sequential只能使用pytorch定义好的一些层结构，不灵活。
# Task4：
**Q1: 回归问题和分类问题在优化器选择上有没有需要注意的地方？**

A1: 感觉优化器和分类还是回归关系不大

**Q2: 如何根据具体任务选择要优化的loss function？**

A2: 这个要根据要解决的问题是回归问题还是分类问题，数据是否不平衡，最后要得到的结果更挂住哪一方面来选取，例如在样本极度不均衡的问题中就不适合准确率，使用auc更好。

**Q3: 优化网络的常用方法有哪些**

A3: 在CNN的网络结构大概就是调整输入图片的分辨率，加深网络或者使用类似inception网络类似的操作使网络加宽。

**Q4: 网络的层数和模型效果是成比例吗？**

A4: 不成比例，网络层数跟模型效果有比例关系，但也不是越大越好

**Q5: 为什么卷积层在图像识别中会有很好的效果？**

A5: :简单看一下卷积操作的原理就可以知道，对图像做卷积变换其实就是对图像不断抽象的过程，卷积层能够提取出图像中对分类有意义的特征。而随着卷积层数加深，这种特征的抽象程度也会越来越高。
# Task5：
**Q1: dropout的概率是否依赖于网络结构，例如卷积层，全连接层的dropout概率是否有推荐的取值**

A1: 一般来说只有全连接层才会使用dropout,dropout的概率本来就是超参数，一般取0.5，也可以自己调整。

**Q2: ：L1、L2和Dropout的异同是什么？防止过拟合还有什么其他的方法呢**

A2: L1,L2正则化通过在损失函数中引入参数，对参数施加惩罚，L1、L2正则化是通过修改代价函数来实现的，而Dropout则是通过修改神经网络本身来实现的，它是在训练网络时用的一种技巧，防止过拟合还可以通过数据增强来实现

**Q3: 权重衰减和丢弃法哪种更常用**

A3: 在神经网络中dropout更常用，正则化在一般的机器学习任务中使用更多

**Q4: 既然算法很多都可以使用numpy和pytorch进行实现，是不是理论上说numpy可以替代pytorch?**

A4: pytorch作为一种深度学习框架，它的优势就是可以在GPU上进行加速，numpy不能在GPU上使用，而且pytorch针对神经网络进行了一系列的优化，这些都是numpy所没有的。

**Q5: 在训练阶段和测试阶段，dropout有何不同？BN层在训练和测试阶段有什么不同，均值和方差是如何计算出来的？**

A5: Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。而在测试时，应该用整个训练好的模型，因此不需要dropout
对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。
而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得。
对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差
# Task6：
**Q1: 优化器对于不同的场景选择上有没有要求？或者说在使用时必须把这几个进行对比？然后取出最优？**

A1: 这个优化器其实只要数据和训练的轮数足够效果是差不多的，不同优化器主要的差别就是收敛速度的不同。

**Q2: 通常来说优化器的参数很重要，那么我们通过什么样的方法可以确定最优参数呢？**

A2: 调参。。。

**Q3: 一定是Adam优化器最好用吗？或者说有没有SGD优化强于Adam的情况？**

A3: 一般来说Adam更稳定，可以看吴恩达关于优化器的部分的讲解
[https://mooc.study.163.com/learn/2001281003?tid=2001391036&_trace_c_p_k2_=7f8ddb27ae2449af854a44c050993fbf#/learn/content?type=detail&id=2001701052](https://mooc.study.163.com/learn/2001281003?tid=2001391036&_trace_c_p_k2_=7f8ddb27ae2449af854a44c050993fbf#/learn/content?type=detail&id=2001701052)

**Q4: 随机梯度下降的特点是什么？怎么实现**

A4: 主要是在theta的更新上，随机梯度下降在更新theta时只是随机选择所有样本中的一个或一个批次，然后对theta求导，所以随机梯度下降具有较快的速度，但是可能陷入局部最优解

**Q5: Adam和RMSprop是否适用于大部分神经网络的训练？**

A5: 是的
# Task7：
**Q1: 网络初始化部分在哪里体现，每次搭建网络都需要重新写网络初始化部分吗？如何写网络初始化部分代码？**

A1: nn.Module模块对于参数进行了内置的较为合理的初始化方式，应该在继承nn.Module以后pytroch就使用了默认的方式进行初始化，当然也可以自定义初始化权值

**Q2: 想问问写了一个基本的，可以从哪些方面来考虑优化，怎么对正确率进行提升？**

A2: 优化的话，数据增强，使用别的网络，调参等等

**Q3: 神经网络解决过拟合的方法有哪些，如何选择？**

A3: 数据增强肯定是防止过拟合的好办法，除此之外，dropout,earlystop,正则化项等

**Q4: 数据集进行正态化处理时，均值0.1307和方差0.3081怎么得来的，会根据数据集的不同而改变吗？**

A4: 个人理解就是整个数据集的均值和方差，是和数据集有关的

**Q5: 以后进一步学习的资源去哪找，有推荐的吗？**

A5: 网上资料很多，google一下


