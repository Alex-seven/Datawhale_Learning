## Task1：
Q1: 我们在选取batch_size的时候，如果样本数除以batch_size还有剩余，也就是说剩余的数量不足一个batch_size，此时pytorch会怎么处理?

A1: 其实batchsize只是批量输入数据而已，就好比批量计算1+2 3+5 4+6 然后突然最后一批只剩下 2+3 这个对于计算没有任何影响。直接把剩下的数据计算完就可以

Q2: net(input)这里是重载了（）吗？为啥可以输入参数呢？

A2: 是因为net继承了nn.Module,重载了__call__方法，在__call__方法中默认调用了forward（)方法,  net(input)默认是调用了net.forward(input)方法。

Q3: pytorch文档中介绍：“Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。”还是不太理解什么是张量？putorch为什么要引入张量？为什么可以在GPU上加速计算？

A3: 张量它和ndarray的一个区别在于它相当于源代码，告诉gpu怎么计算。
它就好比告诉电脑 x+y 这个公式，然后具体的值数据量非常大一次性给清方便批量计算。而ndarray它就好比告诉电脑
1+2
5+6
7+8
这些数据要计算。
用张量有两个好处一来它可以用来自动求导（直接告诉电脑1+2是没法求导的），而来可以gpu根据它进行批量计算。

Q4: pytorch相关深度学习算法的理论部分需要掌握多少，理论部分的掌握程度在实际应用中的影响有多大

A4: 用pytorch搭建深度学习暂时只用了解神经网络的输入格式和输出矩阵形状怎么告诉pytorch。怎么告诉pytorch每层神经网络输入矩阵和输出矩阵的形状。
然后要pytorch怎么向神经网络输入数据，和标签。
要了解训练到底在做什么，训练好了怎么进行预测。
如果是卷积神经网络。那就要了解卷积神经网络中的卷积操作，池化操作怎么做的。怎么在pytorch实现。
 关于如果只想快速学习pytorch就不要纠结梯度下降和反向传播的细节。只用关注怎么设计网络怎么输入数据。

Q5: 误差反向传播是怎么样计算出参数更新值的？

A5: 反向传播就梯度下降的求导部分，它做的工作就是梯度下降求导有些重复计算的结果就直接保存。
所以了解参数更新只需要了解梯度下降就可以。
关于梯度下降可以看这篇文章
https://zhuanlan.zhihu.com/p/43452377