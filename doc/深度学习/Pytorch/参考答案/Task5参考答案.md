##Task5：
Q1: dropout的概率是否依赖于网络结构，例如卷积层，全连接层的dropout概率是否有推荐的取值?

A1: 一般来说只有全连接层才会使用dropout,dropout的概率本来就是超参数，一般取0.5，也可以自己调整。

Q2: ：L1、L2和Dropout的异同是什么？防止过拟合还有什么其他的方法呢?

A2: L1,L2正则化通过在损失函数中引入参数，对参数施加惩罚，L1、L2正则化是通过修改代价函数来实现的，而Dropout则是通过修改神经网络本身来实现的，它是在训练网络时用的一种技巧，防止过拟合还可以通过数据增强来实现

Q3: 权重衰减和丢弃法哪种更常用?

A3: 在神经网络中dropout更常用，正则化在一般的机器学习任务中使用更多

Q4: 既然算法很多都可以使用numpy和pytorch进行实现，是不是理论上说numpy可以替代pytorch?

A4: pytorch作为一种深度学习框架，它的优势就是可以在GPU上进行加速，numpy不能在GPU上使用，而且pytorch针对神经网络进行了一系列的优化，这些都是numpy所没有的。

Q5: 在训练阶段和测试阶段，dropout有何不同？BN层在训练和测试阶段有什么不同，均值和方差是如何计算出来的？

A5: Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。而在测试时，应该用整个训练好的模型，因此不需要dropout
对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。
而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得。
对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差