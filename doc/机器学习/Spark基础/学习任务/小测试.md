### Task1：
**Q1：window也可以布设hadoop？**
windows可以搭建hadoop，不过比较麻烦

**Q2：hadoop在计算方面有什么优于spark的吗？**
spark劣势
稳定性方面，由于代码质量问题，Spark长时间运行会经常出错，在架构方面，由于大量数据被缓存在RAM中，Java回收垃圾缓慢的情况严重，导致Spark性能不稳定，在复杂场景中SQL的性能甚至不如现有的Map/Reduce。
不能处理大数据，单独机器处理数据过大，或者由于数据出现问题导致中间结果超过RAM的大小时，常常出现RAM空间不足或无法得出结果。然而，Map/Reduce运算框架可以处理大数据，在这方面，Spark不如Map/Reduce运算框架有效。

**Q3：spark避免了磁盘的读写，但对内存使用不节制，导致job挂掉，有什么方法避免吗？还是只能通过jvm调参**

**Q4：Spark和hadoop真实工作环境中是否需要一起运用** 

hadoop提供hdfs分布式文件系统,spark作为计算引擎

**Q5：利用python进行spark的的词频统计环境搭建具体流程是怎么样的，我还没配置好，我是用Scala语言进行词频统计的。** 

### Task2：
**Q1：rank如何加入到rdd中的时，如何添加值相等，排名相等的条件？**

**Q2：有没有直接用RDD算子就能直接解决排名的方法**

**Q3：groupBy和groupByKey的区别**

groupBy()方法是根据用户自定义的情况进行分组，而groupByKey()则是根据key值进行分组的，也就是说，进行groupByKey()方法的数据本身就是一种key-value类型的，并且数据的分组方式就是根据这个key值相同的进行分组的，groupBy()分组的方法呢？就是工具groupBy()传入的方法返回的值进行分组

**Q4：共享变量，广播的应用可以理解，累加器的应用是什么？应用场景是什么样子的？**

累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。
例如:使用累加器来对一个整数RDD中的元素个数进行计数，同时使用reduce()动作对RDD中的值求和
val count: Accumulator[Int] = sc.accumulator(0)
val result = sc.parallerize(Array(1, 2, 3))
.map(i => {count += 1, i})
.reduce((x, y) => x+y)
assert(count.value === 3)
assert(result === 6)
当Spark作业的结果被计算出来以后，可以通过累加器调用value来访问它的值

**Q5：持久化是将数据载入到内存嘛？如果有很大量的数据作为中间结果需要怎样操作能提升计算速度呢？**

持久化，也就是cache()并不会直接进行缓存，只是对RDD进行标记，以指示该RDD应当在作业运行时被缓存。持久化就是为了提升计算速度

### Task3：
**Q1：Hive on Spark和Spark SQl区别。Spark SQL的优化问题**

Hive原生是基于MapReduce的，但它现在将不再受限于一个引擎，而是可以采用MapReduce，Spark,，Tez等多种引擎。Hive on Spark 的目的是把Spark作为Hive的一个计算引擎，将Hive的查询作为Spark的任务提交到Spark集群上进行计算。Hive on Spark大体与SparkSQL结构类似，只是SQL引擎不同，但是它们的计算引擎都是Spark。需要注意的是:Hive和SparkSQL本身并不直接参与计算。

**Q2：什么时候使用sparkSql 什么时候使用RDD，他们的效率有什么差别**

RDD可以认为是分布式的Java对象的集合,DataFrame可以看做分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点就是有执行计划的优化器，这样用户只需要指定自己的操作逻辑，DataFrame的优化器会帮助用户选择一条效率最优的执行路径。同时Tungsten优化使得DataFrame的存储和计算效率比RDD高很多。Spark的机器学习项目MLlib的ML pipeline就是完全基于DataFrame的，而且未来Streaming也会以DataFrame为核心。

**Q3：什么时候使用DataFrame的API比较好？**

在spark中。dataframe类似于传统数据库中的二维表格，dataframe带有schema信息，可以直接针对hdfs等任何可以构建为RDD的数据，可以通过spark sql查看更多的结构信息

**Q4：共Spark与其他流计算框架有什么区别，最大的优势在哪里？**

**Q5：为什么要把rdd转化成dataframe的形式进行计算？**

在spark中。dataframe类似于传统数据库中的二维表格，dataframe带有schema信息，可以直接针对hdfs等任何可以构建为RDD的数据，可以通过spark sql查看更多的结构信息

### Task4：
**Q1：对于tensorflow和pytorch这类工具对于大量数据可以使用分批加载数据的方式进行训练，那么spark.ml相对于这两种工具的优势在哪？**

**Q2：如何更好地评估无监督学习的效果？**

无监督学习的衡量没有标准方法，具体的应用会导致不同的评估方法。一般来说，无监督学习主要是为了挖掘数据中的局部关系或者局部和全局之间的关系，往往评估方法也都是依赖于密度或距离、统计学手段、监督学习的方法。
从聚类来看，距离度量和密度估计是比较直观的判断方法。异常检测也有类似的性质，但往往比聚类更难（因为数据不平衡）。
除了距离度量，互信等类似的手段也可以用作衡量相似度和差异度。如果对数据和问题有更深刻的理解，还可以假设概率分布。虽然无监督学习没有标签，但从基本理论上依然可以借鉴很多监督学习的思路，也包括评估方法。某些特定情况下，我们可以尝试生成标签，将问题转化为监督学习。
正因为无监督学习面临如此复杂的场景，所以比较可靠的手段是尝试使用集成学习，对没有标签的数据使用集成算法。
详情请参照
一个无监督学习算法，如何判断其好坏呢? - 微调的回答 - 知乎 https://www.zhihu.com/question/30855292/answer/266430108

**Q3：用spark做机器学习有什么优势吗？**

1. Spark ml参照sklearn pipline设计的，上手更容易
2. Spark提供了大数据处理能力
3. Spark生态也很好，主流的模型都有lgb xgb，甚至是nn

**Q4：如何把神器xgboost放在spark上运行**

1. Xgb spark版
2. H2o spark版，内置xgb
3. Python/java/scala训练好的模型spark也能直接加载
等等

### Task5：
**Q1：ml 和 mllib 两个包之间相同的模块，他们的代码原理是一样的，怎么会有格式不兼容，是有什么区别吗，例如Vector和Vectors**

spark.mllib包含基于RDD的原始算法API。在1.0 以前的版本即已经包含了，提供的算法实现都是基于原始的 RDD，3.0以后会移除。
spark.ml 则提供了基于DataFrames 高层次的API，可以用来构建机器学习工作流（PipeLine）。ML Pipeline 弥补了原始 MLlib 库的不足，向用户提供了一个基于 DataFrame 的机器学习工作流式 API 套件。
所以新的代码应尽量采用ml库。

**Q2：spark ml好像没有sklearn好用,但是实际上是不是大数据集只能在spark做**

spark ml好像没有sklearn好用,但是实际上是不是大数据集只能在spark做
数据可以分批加载呀

**Q3：效果比较好的逻辑回归算法的优化有哪些？**

**Q4：spark和tensorflow相比用来做机器学习有什么优势？**