# 目录
**Task1**
Q1: boosting方法里一系列的弱学习器都是同一种吗？可不可以在一个boosting方法里采用多种弱学习器；如果可以这样做，举例来说，感知机分类错误的样本为什么值得下一个学习器svm多加注意？
Q2: 决策树具有一定的可解释性，随机森林是否也有这种特性？或者说，能不能从生成树看出某些规则？
Q3: 在集成算法中，bagging算法主要通过降低模型的variance来提升模型效果，boosting算法通过降低bias来提升模型效果，那么有没有一种算法可以结合两者的优点？将随机森林内的决策树改为提升树？数据采样方式更加注重保留原有的数据结构？
Q4: 随机森林在解决回归问题时，为什么不能给出连续的输出？
Q5: 当算法出现过拟合的时候怎么处理？怎么解决高方差和高偏差的问题？

**Task2**
Q1: 提升树模型是如何构成树模型的？GBDT的损失函数是不是都用平方差？
Q2: 通俗的解释adaboost，gbdt，xgboost区别
Q3: GBDT与随机森林哪种算法运用在特征重要度比较常见，优势是什么
Q4: 为什么说负梯度就是残差，对于回归问题，我们要拟合的就是残差？
Q5: 为什么在诸多机器学习算法（不包括深度学习）中，boosting通常能够取得好的效果？

**Task3**
Q1: 面试时，需要准备xgboost算法哪些内容？例如xgboost与gbdt的对比，xgboost为何需要二阶展开等？
Q2: XGBoost怎么学习缺失值处理的 
Q3: 相对于GBDT，惩罚项起到了什么作用，导致效果提升明显。 
Q4: xgboost等树模型对于特征的量纲不敏感，造成这个特性的理论层面的原因是什么？
Q5: 1.分裂结点算法理解不够透彻，为啥就二阶展开了
2.xgb快在哪里，为什么适合处理大量数据？
3.参数很多，求调参经验 
Q6: 这个是怎么推出来的？
![图片](https://uploader.shimo.im/f/CEgADsT0vJsVrSWK.png!thumbnail)
**Task4**
Q1: 看起来lightGBM 比XGBoost优秀太多，那为什么XGBoost未被取代依然很火，所以XGBoost有哪些无法取代的优势
Q2: 学了这么多的梯度提升的算法，现在大家最为推崇的是哪一种呀？另外工业界对哪个比较青睐呀？
Q3: 直方图算法把数据集分为k个桶，当桶数较小时，对这k个桶进行迭代和计算，结果会不会不太精确。另外，通过设置树的最大深度来避免过拟合，那怎样判断一个较好的深度？能不能用最小分裂样本值呢？
Q4: lightgbm有这么多优点，是否别的算法可以弃用？只要有大量数据，深度学习效果更好，那是不是实际工作中都用tensorflow或其它工具来解决问题？
Q5: 我新构建的特征在LightGBM的feature importance上排在前面，但最后得分反而下降了，是什么原因呢？

# Task1：随机森林算法梳理
**Q1: ****boosting方法里一系列的弱学习器都是同一种吗？可不可以在一个boosting方法里采用多种弱学习器；如果可以这样做，举例来说，感知机分类错误的样本为什么值得下一个学习器svm多加注意？**
A1: 弱学习器不是同一种，可以采用多种。boosting根据前一个学习器的训练效果对样本分布进行调整，对分类错误的样本更为关注，再根据新的样本分布训练下一个学习器，如此迭代M次，最后将一系列弱学习器组合成一个强学习器。原理主要是通过改变错误样本所占的权值来改变分类边界，从而一步步提升算法的准确度。

**Q2: ****决策树具有一定的可解释性，随机森林是否也有这种特性？或者说，能不能从生成树看出某些规则？**
A2: 决策树的可解释性主要就是特征重要性和分支点的值，在随机森林里特征重要性可以对所有树进行平均，个人认为可能比单个树更加客观，分支点的值可以分组求和平均，同时由于有了样本基数，这些都具有统计意义了，很多统计分析方法都可以用上，所以感觉随机森林可解释性可能比单个数更强。（by 琛子豪）
随机森林也是一种bagging思想，它通过每次随机采样使得训练样本不同，从而获得的每个决策树不同，然后采用加权投票获得最终的分类结果，可以通过每个决策树的生成规则找出不同的特征，个数最多的特征为最显著特征。（by 一切顺利）

**Q3: ****在集成算法中，bagging算法主要通过降低模型的variance来提升模型效果，boosting算法通过降低bias来提升模型效果，那么有没有一种算法可以结合两者的优点？将随机森林内的决策树改为提升树？数据采样方式更加注重保留原有的数据结构？**
A3: 目前感觉没有一种算法既可以降低偏差和方差，因为本身就是两个优化目标。（by 一切顺利）

**Q4: ****随机森林在解决回归问题时，为什么不能给出连续的输出？**
A4: 因为每个节点的输出是固定的，节点数量是有限的，所以输出是不连续的，如果在叶结点内拟合模型（model tree, 在scikit-learn内有PR），那输出就是连续的。（by 秦汉民）

**Q5: ****当算法出现过拟合的时候怎么处理？怎么解决高方差和高偏差的问题？**
A5:过拟合方法很多，只能降低，一是增加训练集，如数据增强，二是降低特征数量或者特征所对应的系数，如L1和L2正则化，三是增大正则化参数，降低模型复杂度；四是神经网络中加入dropout等等，解决高方差就是解决过拟合问题，解决高偏差就是解决欠拟合…（by 一切顺利）
# Task2：GBDT算法梳理
**Q1: ****提升树模型是如何构成树模型的？GBDT的损失函数是不是都用平方差？**
A1: GBDT论文定义了其决策树的训练标准，在scikit-learn中称为friedman_mse，GBDT分类问题和回归问题可以使用不同的损失函数，具体参见GBDT原始论文以及scikit-learn支持的损失函数。

**Q2: ****通俗的解释adaboost，gbdt，xgboost区别**
A2: adaboost是boosting算法的一种，它的基本思想是利用前一轮弱学习器的误差来更新训练样本的权重，从而使得分错样本权重变大，分对样本权重变小，而初始的时候所有训练样本的权重相等，具体来说adaboost使用的是每一个弱学习器的权重求指数来更新训练样本的权重，对于分错样本，权重为弱学习器权重的正指数，对于分对样本，权重为弱学习器权重的负指数，不断经过迭代，后一个学习器不断修正前一个学习器的误差，直到误差达到最小值，且adaboost的训练误差上界是趋于0的，也就是训练误差基本为0；gbdt是boosting算法的另一种，与adaboost不同，gbdt首先规定弱学习器为cart回归树模型，其次损失函数可以是任意的，通过用损失函数的负梯度拟合本轮损失的近似值，从而拟合一个cart回归树；xgboost是gbdt的工程实现，对gbdt算法进行了一定的改进，使用二阶导数，目标函数中引入正则项减少方差，测试的时候可以并行训练等等...（by 一切顺利）

**Q3: ****GBDT与随机森林哪种算法运用在特征重要度比较常见，优势是什么**
A3: 随机森林可以用袋外数据错误率或者Gini系数评价指标来评估特征重要度，GBDT则主要采用Gini系数。具体内容见[https://blog.csdn.net/tinkle181129/article/details/80231871](https://blog.csdn.net/tinkle181129/article/details/80231871)

**Q4: ****为什么说负梯度就是残差，对于回归问题，我们要拟合的就是残差？**
A4: 梯度是一个向量，它的大小是最大变化率，方向是最大方向导数的方向，负梯度是最大变化率的负数，也就是梯度下降方向，残差也就是上一轮损失值与本轮损失值的差，那么我们希望残差下降的越快越好，及残差的变化率最大，就对应着负梯度，而损失函数的负梯度就可以获得损失函数的最小值。（by 一切顺利）

**Q5: ****为什么在诸多机器学习算法（不包括深度学习）中，boosting通常能够取得好的效果？**
A5: 我觉得bagging的思想就是三个臭皮匠赛过诸葛亮，它要求基学习器之间越不同，效果越好，最终通过聚合每一类投票数来决定最终的分类类别，它没有一个优化目标是它最大的弊端，不能使用迭代策略，是一种投票决策机制；而boosting是一种迭代学习器，它有目标函数，我们可以使用一些优化算法使得最终的分类或回归任务倾向于我们想要的，而不是像bagging那样没有目标的决策，决策出来的结果不一定都是好的。（by 一切顺利）
为什么多模型能带来更好效果呢？
《机器学习》（周志华）一书从三个角度给出了学习器的结合带来的三个好处：
从统计上来说，由于假设空间很大，可能有多个假设能在训练集达到同等最优性能（但对于测试集表现不同），若使用单个学习器可能因误选使泛化性能不佳，因此多个学习器可减少这一风险
从计算性能（最优化）来看。学习算法可能陷入局部极小，因此泛化性能不好。多学习器可降低风险
从表示的方面看，某些学习任务的真实假设可能不在当前学习算法考虑的假设空间，多个学习器则扩大来假设空间。
另外自己再添加一点：Boosting 类算法能够利用之前的基础学习器的结果，给予错误分类样本更大的权重，或者是利用残差学习。降低了模型的偏差，因此效果更好。参考[https://www.cnblogs.com/sarahp/p/6899906.html](https://www.cnblogs.com/sarahp/p/6899906.html)
# Task3：XGB算法梳理
**Q1: ****面试时，需要准备xgboost算法哪些内容？例如xgboost与gbdt的对比，xgboost为何需要二阶展开等？**
A1: 可在网上搜索xgboost算法常见面试题。在XGBoost中泰勒展开的作用是去逼近损失函数，转化为多项式函数的话，后续的求导变得方便了。二阶泰勒展开对于常用的几种损失函数来说逼近效果都已经足够好了，所以模型采用了二阶展开。（by 张雨）

**Q2: ****XGBoost怎么学习缺失值处理的 **
A2: XGBoost并不会学习处理缺失值，因为缺失值是信息丢失的，没有信息就没办法学习，xgboost只是把缺失值样本归到了更可能的那一类，当然这是不严谨的。（by 张雨）

**Q3: ****相对于GBDT，惩罚项起到了什么作用，导致效果提升明显。 **
A3: 惩罚项起到了防止过拟合的作用，惩罚项里的T是叶子节点的个数，叶子节点越多，分的越细，也就越容易学到噪音造成过拟合，所以要限制回归树不要分得太细，惩罚项里回归结果的范数也是起到限制叶子节点个数的作用。（by 张雨）

**Q4: ****xgboost等树模型对于特征的量纲不敏感，造成这个特性的理论层面的原因是什么？**
A4: 因为树类模型不管是ID3还是C4.5还是CART，在进行分裂的时候主要关心的是特征的取值分布和取值之间的条件概率，对取值本身并不敏感。（by 潘华引Simon Pan）

**Q5: ****1.分裂结点算法理解不够透彻，为啥就二阶展开了**
**2.xgb快在哪里，为什么适合处理大量数据？**
**3.参数很多，求调参经验 **
A5: 1.在XGBoost中泰勒展开的作用是去逼近损失函数，转化为多项式函数的话，后续的求导变得方便了。二阶泰勒展开对于常用的几种损失函数来说逼近效果都已经足够好了，所以模型采用了二阶展开。（by 张雨）
2.GBM慢在每一步选特征，xgboost速度快是因为在训练之前，预先对数据进行排序，然后保存block结构，后面的迭代中重复的使用这个结构，大大减小计算量。(by 瑶瑶）
我记得陈大佬的论文里面提了一句，关于分裂节点算法，目前BGDT所实现的都是Basic Exact Greedy Algorithm，虽然很强大，但是计算量非常大，而且在内存不够大的时候无法高效运行，在分布式计算的时候也有一些问题。而XGBoost同时实现了支持分布式、效率更高的Approximate Algorithm，所以更快，更适合处理大量数据。具体可以看看论文的第三部分。（by 潘华引Simon Pan）
3.从比赛层面来说，其实调参数关系不大，主要还是特征层面的问题。初步做参数的话，推荐较低的learning rate，6-10的max-depth，subsample 0.8-1，其他基本不动（by bboy_lowe1）

**Q6: ****这个是怎么推出来的？**
![图片](https://uploader.shimo.im/f/CEgADsT0vJsVrSWK.png!thumbnail)
A6: 这个是根据牛顿迭代近视法，在XGBoost里面求每个叶子节点最优权重的时候其实也用到了这种近视，从论文的方程4到方程5，就是将方程4对Wj求导，并令导数为0，可以得到Wmj=-Gmj/(Hmj+lambda)，其中Gmj是损失函数的一阶导求和，Hmj是二阶导求和，lambda是正则项。对于GBDT的二分类来说，近视 结果是一样的，而且因为没有正则项，结果更简单，每个叶子节点的最优值就是 Wmj= - Gmj/Hmj，这里的Wmj就是GBDT论文中的\gamma_{mj}，然后你把二阶导数推一下应该就能够出来了。（by 潘华引Simon Pan）
![图片](https://uploader.shimo.im/f/IDtBRSKB7S8fipfX.png!thumbnail)
# Task4：LightGBM算法梳理
**Q1: ****看起来lightGBM 比XGBoost优秀太多，那为什么XGBoost未被取代依然很火，所以XGBoost有哪些无法取代的优势**
A1: 目前lightGBM的资料文档还比较少，使用不够普遍。另外xgboost本身已经有很好的特性，比如默认xgboost是可以自动处理缺失值的，还有xgboost特征划分使用的近似分位数方法也是一种直方图，在大数据集上性能也很好。

**Q2: ****学了这么多的梯度提升的算法，现在大家最为推崇的是哪一种呀？另外工业界对哪个比较青睐呀？**
A2: GBDT和XGBoost再工业界使用比较普遍。

**Q3: ****直方图算法把数据集分为k个桶，当桶数较小时，对这k个桶进行迭代和计算，结果会不会不太精确。另外，通过设置树的最大深度来避免过拟合，那怎样判断一个较好的深度？能不能用最小分裂样本值呢？**
A3: 是会丢失一些精度，不能像pre-sorted找到最精确的分割点，但是直方图算法带来的好处是更大的，比如构建直方图的时候就会计算梯度，不需要数据id到叶子号的索引，没有了cachemiss问题，优化了时间复杂度，降低了计算、通信等效率的代价；判断树深度应该只能慢慢调了吧，不过对于lgb应该是使用num_leaves，而不是树深度，有个公式num_leaves <= 2^(max_depth) （by superego）

**Q4:**** ****lightgbm有这么多优点，是否别的算法可以弃用？只要有大量数据，深度学习效果更好，那是不是实际工作中都用tensorflow或其它工具来解决问题？**
A4: 不是的，很多基础的算法之所以没有被淘汰，是因为他自身有一些深度学习算法无法比拟的优势，比如线性回归模型的可解释性就比深度学习好太多，而且在很多情况下，数据量不够大，问题本身也没有那么复杂的时候，盲目使用深度学习反而会造成模型过拟合、花费时间长却收效甚微的情况。

**Q5: ****我新构建的特征在LightGBM的feature importance上排在前面，但最后得分反而下降了，是什么原因呢？**
A5: 从我的经验来看，新构建的模型很有可能与原特征高度相关，建议将与之高度相关的特征删除。我的建议是，如果特征特别多，可以先将重要性靠后的特征，做嵌入，比如将这部分特征与Y做一次拟合，那么就可以得到一个概率值，这个概率值作为新的特征，与特征重要性较高的特征合并再做拟合，那么既可以学习到这部分特征的数据，也能降低过拟合的可能性。（by bboy_lowe1）


