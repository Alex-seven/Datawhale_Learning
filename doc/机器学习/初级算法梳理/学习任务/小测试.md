# 目录
**Task1**
Q1：线性回归算法常用于解决什么问题？一般情况下参数如何确定和优化？
Q2：损失函数在多种问题模型下如何进行选择？ 在维度较高的模型，线性回归模型过于简单时，使模型一直处于欠拟合状态时，我们应该采取维度约减的方法减少维度，还是采用更加复杂的学习模型？
Q3：L1正则和L2正则有什么区别？什么场景下使用？
Q4：梯度下降法的最优解属于全局最优还是局部最优？
Q5：l1正则在0点不可导，能否用梯度下降，bfs ,lbfs,adam等。应该是不能，adam 逻辑回归都要求导数，那如果不行，怎么处理呢？

**Task2**
Q1：（1）逻辑回归，从本质上就是线性回归外嵌了sigmod函数，把映射范围离散化（二值化），可以认为原始的逻辑回归只能对于问题模型成线性相关的类型做出分类，无法直接作用于非线性问题，有哪些方法与逻辑回归模型融合来处理非线性回归，比如SVM中采用核函数来解决这个问题。
（2）在逻辑回归中与线性回归的明显差异也可以体现在损失函数的构造上，对于线性模型的MSE等损失函数，逻辑回归采用后，会导致优化问题变为非凸问题，优化难度太大，才采用最大似然估计的来构造损失函数，问题是非凸问题和凸问题如何判断，以及是否存在其他的类似最大似然估计的方式构造容易解的优化损失函数？
（3）正则化项在构造时，一般采用的是模型参数向量的范数，正则化项的本身含义是一个与模型难度成正相关的表达式，作用对象是否可以变为其他对象，有具体的实例吗？
Q2：（1）如果分类类别数目>2，应采用哪种分类模型？（2）logistic回归和感知机有啥区别？
Q3：scikit-learn的逻辑回归为什么将多分类问题的默认解法由ovr换为multinomial
Q4：精确率，准确率，召回率在不同场合下如何分析模型优劣？
Q5：不平衡样本，重采样会不会造成data损失，会有overfitting的问题

**Task3**
Q1：在决策树剪枝处理中，如何判断应该应用前剪枝还是后剪枝
Q2：除了决策树剪枝之外，在决策树生成的过程中是否有一些指标或者方法来防止过拟合问题？
Q3：CART回归树的生成中如何选择最优切分变量j与切分点s？
Q4：构建决策树过程中哪个环节最耗时?
Q5：如何解决决策树的不稳定性问题?

# Task1：线性回归算法梳理
**Q1****：线性回归算法常用于解决什么问题？一般情况下参数如何确定和优化？**
A1： 线性回归解决回归类问题， 参数通过梯度下降随机梯度下降，bfs等优化算法优化。

**Q2****：损失函数在多种问题模型下如何进行选择？**** ****在维度较高的模型，线性回归模型过于简单时，使模型一直处于欠拟合状态时，我们应该采取维度约减的方法减少维度，还是采用更加复杂的学习模型？**
A2：线性回归模型过于简单时，使模型一直处于欠拟合状态时， 一种方法应该是通过人工特征组合，找到更多有效的特征吧。 或者使用复杂模型。

**Q3****：****L1****正则和****L2****正则有什么区别？什么场景下使用？**
A3：正则化的使用场景是为了降低过拟合，L1范数正则化项是向量中各个元素的绝对值之和。L1范数可以实现让参数矩阵稀疏，让参数稀疏的好处，可以实现对特征的选择（权重为0表示对应的特征没有作用，被丢掉），也可以增强模型可解释性（例如研究影响疾病的因素，只有少数几个非零元素，就可以知道这些对应的因素和疾病相关），L1又称Lasso。
L2范数是指向量各个元素的平方，求和，然后再求平方根。使L2范数最小，可以使得W的每个元素都很小，都接近于0，但和L1范数不同，L2不能实现稀疏，不会让值等于0，而是接近于0。一般认为，越小的参数，模型越简单，越简单的模型就不容易产生过拟合现象。
使用场景的话：使用L1范数，可以使得参数稀疏化，方便计算，但是没有考虑到全局特性；使用L2范数，倾向于使参数稠密地接近于0，避免过拟合。优点是当你L1正则化时，没有把噪点值置为0，此时由于比较稀疏，噪点对模型影响会比较大，但是L2正则化考虑每个点，对于会将噪点影响减弱（类似的可以想一下求平均值），鲁棒性会好很多。具体使用的话可以结合优缺点和具体的场景进行选择，当然最直接就是就都试一下，选择哪个效果好就用哪个。

**Q4****：梯度下降法的最优解属于全局最优还是局部最优？**
A：如果损失函数是凸函数，那梯度下降最优解可以到全局最优解，如果不是凸函数，就是局部最优。在高维的特征中，往往局部最优就是全局最优了。

**Q5****：****l1****正则在****0****点不可导，能否用梯度下降，****bfs ,lbfs,adam****等。应该是不能，****adam ****逻辑回归都要求导数，那如果不行，怎么处理呢？**
A5：确实在0点不可导，但是提出一个次梯度的方法用于代替梯度，次梯度可以使用梯度下降（有些许区别）。bfs,lbfs,adam就不知道能不能用了。
** **
# Task2：逻辑回归算法梳理
**Q1****：（****1****）****逻辑回归，从本质上就是线性回归外嵌了****sigmod****函数，把映射范围离散化（二值化），可以认为原始的逻辑回归只能对于问题模型成线性相关的类型做出分类，无法直接作用于非线性问题，有哪些方法与逻辑回归模型融合来处理非线性回归，比如****SVM****中采用核函数来解决这个问题。**
**（****2）****在逻辑回归中与线性回归的明显差异也可以体现在损失函数的构造上，对于线性模型的****MSE****等损失函数，逻辑回归采用后，会导致优化问题变为非凸问题，优化难度太大，才采用最大似然估计的来构造损失函数，问题是非凸问题和凸问题如何判断，以及是否存在其他的类似最大似然估计的方式构造容易解的优化损失函数？**
**（****3****）****正则化项在构造时，一般采用的是模型参数向量的范数，正则化项的本身含义是一个与模型难度成正相关的表达式，作用对象是否可以变为其他对象，有具体的实例吗？**
A1：（1）逻辑回归只是对线性回归的计算结果加上一个Sigmoid函数，将特征线性求和结果转化为了0到1之间的概率。其函数形式是非线性的，但本质上是基于线性回归模型进行的非线性映射，且决策边界(decisionboundary)由输入的数据形式所决定，原始数据特征之间为线性分，则决策面采用线性模型进行拟合；若原始数据特征为非线性，则决策面也为非线性。逻辑模型实际上为广义线性模型，线性分类器，无隐层的最简单的神经网络。（补：逻辑斯蒂回归能否解决非线性分类问题？[https://www.zhihu.com/question/29385169](https://www.zhihu.com/question/29385169)）
（2）[机器学习中关于判断函数凸或凹以及最优化的问题](http:// https://blog.csdn.net/xmu_jupiter/article/details/47400411) 
（3 xgboost里用树的个数作为正则化约束。

**Q2****：（****1****）如果分类类别数目****>2****，应采用哪种分类模型？（****2****）****logistic****回归和感知机有啥区别？**
A2：（1）逻辑回归在处理多分类问题时候，目前我知道的有加入 one vs all机制，还有加入softmax，都可以做多分类。（我的笔记里有点点提及，也有扩展阅读链接）（2）我的认识是感知机是把线性模型的截距移到了0值，而逻辑回归其实是把每个点都有概率的映射，就像一个概率密度函数一样，感知机只是判断大小，没有概率的这个处理。

**Q3****：****scikit-learn****的逻辑回归为什么将多分类问题的默认解法由****ovr****换为****multinomial**
A3：在机器学习中, 多类(*multiclass*)或者多项式(*multinomial*)分类是将实例分配给一个而非多于两个类别的种类(将实例分类给两类中的一个称为二元分类*binary classification*). 很多分类算法自身支持多于两类的使用, 剩下的就是二元分类算法了, 这就可以通过很多策略去转换成多项式分类器. 要将多类分类与多标签分类区分开, 后者是一个类别有多个标签需要被预测。

**Q4****：精确率，准确率，召回率在不同场合下如何分析模型优劣？**
A4:要根据业务场景和数据情况来看用什么评估标准了，如果数据非常不均衡那是用准确率就不对了，如果要求判断结果坚决不能出错，那可能准确率好些，如果比如希望减少长尾，可能召回率好些。

**Q5****：不平衡样本，重采样会不会造成****data****损失，会有****overfitting****的问题**
A5：不均衡样本 如果样本量非常大可以考虑下采样，如果样本量小，可以考虑上采样。上采样我觉得有overfitting的风险。往往解决过拟合的方法的一般方式是增加训练样本，如果训练样本过少的话，就可能带来过拟合。
[从重采样到数据合成：如何处理机器学习中的不平衡分类问题？](https://www.cnblogs.com/huanjing/p/6789731.html) 

# Task3：决策树算法梳理
**Q1****：在决策树剪枝处理中，如何判断应该应用前剪枝还是后剪枝**
A1: 后剪枝决策树在数学上更加严谨，得到的树至少是和预剪枝得到的一样好。
剪枝方法的选择：如果不在乎计算量的问题，后剪枝策略一般更加常用，更加有效。后剪枝中1和2通常需要训练集和额外的验证集，计算量更大。有研究表明，通常错误率降低剪枝（REP,Reduce-Error Pruning）剪枝是效果最好的，但是也不会比其他的好太多。经验表明，限制节点的最小样本个数对防止过拟合很重要，输的最大深度的设置往往要依赖于问题的复杂度，另外树的叶节点总个数和最大深度是相关的，所以有些设置只会要求指定其中一个参数。

**Q2****：除了决策树剪枝之外，在决策树生成的过程中是否有一些指标或者方法来防止过拟合问题？**
A2：首先分析一下产生过度拟合数据问题的原因有哪些？
原因1：样本问题
   （1）样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；
   （2）样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或业务场景；
   （3）建模时使用了样本中太多无关的输入变量。
原因2：构建决策树的方法问题
   在决策树模型搭建中，我们使用的算法对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据或非事件数据，可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。
如何解决过度拟合数据问题的发生？  
针对原因1的解决方法：
    合理、有效地抽样，用相对能够反映业务逻辑的训练集去产生决策树；
针对原因2的解决方法（主要）：
    剪枝：提前停止树的增长或者对已经生成的树按照一定的规则进行后剪枝。

**Q3****：****CART****回归树的生成中如何选择最优切分变量****j****与切分点****s****？**
A3:在训练集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树； 
选择最优切分变量j与切分点s，求解 ![图片](https://uploader.shimo.im/f/t1k5dm5BKqoBxABl.png!thumbnail)
遍历变量j，对固定的切分变量j扫描切分点s，选择使上式达到最小值的（j,s） 

**Q4****：构建决策树过程中哪个环节最耗时****?**
A4：决策树的构建过程：
根据训练集，从根节点开始，递归地对每个节点进行以下操作，构建二叉决策树： 
（1）设结点的训练数据集为D，计算现有特征对该数据集的切分误差，此时，对于每一个特征A,对其可能取的每个值a，据此进行样本二分，计算两个子样本中标签项的方差之和。 
（2）将所有可能得特征值A以及它们所有可能得切分点a中，选择样本划分之后方差和最小的值作为最优切分点，该值对应的特征为最优特征。依最优特征和最优切分点，从现结点生成两个子结点，将训练数据依特征分配到两个子结点中去。 
（3）对两个子结点递归地调用（1），（2），直至满足停止条件 
（4）生成CART决策树
整个过程中一直在重复的执行(1)(2)，在(1)(2)两个步骤中，最费时的应该是遍历寻找最优切分点。

**Q5****：如何解决决策树的不稳定性问题****?**
 A5：决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。




