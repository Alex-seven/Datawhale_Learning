# 目录
**Task1**
Q1: 使用urllib和request爬虫的区别是？以及两者效率上有差别吗？
Q2: urllib和reques两个包哪个更常用？怎么确定哪种网页的爬取要用哪种方法更好呢？
Q3: 为什么存成csv格式的时候，中文字符会变成乱码，打开文件时已经用了utf-8编码了？
Q4: 在pycharm 中requests安装不成功怎么回事，  Could not find a version that satisfies the requirement requests (from versions: )No matching distribution found for requests？
Q5: post和get按道理获取数据是一样的，只是数据大小有不同，但为什么这里用post获取只能获取第一页数据？为什么函数要写在前面，不能像JAVA一样，写在任何一个地方吗？

**Task2**
Q1: 如何翻页获取数据，beautifulsoup中.text和get_text()的区别是什么？
Q2: 如何处理抓取内容的回车和空格？
Q3: 多页内容如何抓取？
Q4: 这两个解析库，大概思想感觉是一样的，就是使用方法不太一样，我们需要把这些都学会吗?是不是熟悉掌握一种就可以了呢？

**Task3**
Q1: 怎么在ip被封之后实现自动更换代理池内的代理？
Q2: 如何用一句通俗的语言解释清楚request、beautifulsoup和selenium三者与浏览器之间的关系？
Q3: 构建好代理池后，如果在一次爬虫中自动切换代理？ 比如代理无效，或者代理ip被封，这时自动切换下一个ip。
Q4: ip_list.append(f'{protpcol}://{ip}:{port}')这里的f是格式化？

**Task4**
Q1: 图形验证码应该如何解决
Q2: 验证码问题一般怎么实现？只有打码平台一种方法吗？这些图像码是自动生成的还是固定的图片？使用代理后是不是就不能使用Cookie方式登录了？
Q3: .还是图片验证码的问题没有解决2. xpath遇到回复拆分为了多个对象，使用posts[i].xpath('string()').strip()解决了，如果回复直接获取text()，有没有办法解决数据拆分的问题呢
Q4: selenium的find_element_by_name里面的name怎么找呢
Q5: 爬虫反扒验证码解决途径？

# Task1：学习get与post请求和正则表达式
**Q1: 使用urllib和request爬虫的区别是？以及两者效率上有差别吗？**

A1: urllib和urllib2模块都做与请求URL相关的操作，但他们提供不同的功能。 
urllib2.urlopen accepts an instance of the Request class or a url, （whereas urllib.urlopen only accepts a url 中文意思就是：urllib2.urlopen可以接受一个Request对象或者url，（在接受Request对象时候，并以此可以来设置一个URL的headers），urllib.urlopen只接收一个url 
urllib 有urlencode,urllib2没有，这也是为什么总是urllib，urllib2常会一起使用的原因

**Q2: urllib和reques两个包哪个更常用？怎么确定哪种网页的爬取要用哪种方法更好呢？**

A2: 1、Requests  使用的是 urllib3，继承了urllib2的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的 URL 和 POST 数据自动编码。 requests不是python自带的库，需要另外安装 easy_install or pip install，requests缺陷:直接使用不能异步调用，速度慢（from others）。官方的urllib可以替代它，个人不建议使用requests模块。

**Q3: 为什么存成csv格式的时候，中文字符会变成乱码，打开文件时已经用了utf-8编码了？**

A3: Excel另存CSV文件不能选择UTF-8编码，原因如下：
1. Excel主要用来处理数据。CSV，逗号分隔值文件格式，逗号分隔值，是因为分隔字符也可以不是逗号。例如TXT就是制表符分隔。Excel支持很正常。
2. UTF-8属于字符编码。UNICODE支持欧洲、非洲、中东、亚洲（包括统一标准的东亚像形汉字和韩国像形文字）。虽大而全却并不高效，于是出现通用转换格式，即UTF（Universal Transformation Format）。目前存在的UTF格式有：UTF-7,UTF-7.5,UTF-8,UTF-16，以及 UTF-32。
3. 由于字符编码种类非常多，因此Excel有选择性的支持了ANSI(微软自已的)和UNICODE（大而全）,不支持UTF-8很正常。记事本支持UTF-8编码。

要解决Excel文件保存为CSV格式且采用UTF-8编码可以通过记事本来转换，方法如下

1. 首先将Excel文件保存为CSV格式
2. 将CSV格式在记事本中打开
3. 选择另存为，同时将“编码(E)"选择为 UTF-8
4. 点 保存(S),就达到目的了

**Q4: 在pycharm 中requests安装不成功怎么回事，  Could not find a version that satisfies the requirement requests (from versions: )No matching distribution found for requests？**

A4: 找不到满意的版本，这时就是我们的pip可能需要升级了，所以使用python -m pip install --upgrade pip升级一下pip ，--upgrade 后面跟的是需要升级的库名然后继续尝试发现还是不行，会报相同的错误，这时考虑到是网络的问题，我的网有时候是不稳定的，这时我们用国内的镜像源来加速。

**Q5: post和get按道理获取数据是一样的，只是数据大小有不同，但为什么这里用post获取只能获取第一页数据？为什么函数要写在前面，不能像JAVA一样，写在任何一个地方吗？**

A5: post一般不会显示参数，一定要用get，不会隐藏参数，但你可以加密后传输，然后后台解密后使用。也可以不写在前面。

# Task2：学习beautifulsoup和学习xpath
**Q1: 如何翻页获取数据，beautifulsoup中.text和get_text()的区别是什么？**

A1: beautifulsoup中，对外接口，没有提供text这个属性，只有string这个属性值；
beautifulsoup内部才有text这个属性，只供内部使用 –> 如果你想要用text值，应该调用对应的get_text()，而你之所有能够直接用soup.text而没报错，应该是和python的class的property没有变成private有关系 –>导致你外部也可以访问到这个，本身是只供内部使用的属性值-> 这个要抽空深究了。

**Q2: 如何处理抓取内容的回车和空格？**

A2: [EL表达式](https://www.baidu.com/s?wd=EL%E8%A1%A8%E8%BE%BE%E5%BC%8F&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)会将"</br>"[换行符](https://www.baidu.com/s?wd=%E6%8D%A2%E8%A1%8C%E7%AC%A6&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)转变成"</br>","<"和">"分别对应"<"和">",也就是说，到了页面上,还是显示为"</br>"，为了补救这一点,需要使用jsp标签(<%= %>)来进行显示。所以当需要在普通文本域中显示由<textarea>提交的内容时,可以直接在数据库里存储不经过处理的字符串,而只在显示前将"\r\n"替换为"</br>",然后在页面上使用<%= %>来显示这些内容. 

**Q3: ****多页内容如何抓取？**

A3: 用for循环，实现所有地址的遍历，也可以增加一个列表，将所有地址直接插入列表中，抓取时直接调用。

**Q4: ****这两个解析库，大概思想感觉是一样的，就是使用方法不太一样，我们需要把这些都学会吗?是不是熟悉掌握一种就可以了呢？**

A4: Xpath 必然 是要比 BeautifulSoup 在时间和空间上都要性能更好一些。其中理由有很多，其中一个很明显的是 BeautifulSoup 在构建一个对象的时候需要传入一个参数以指定解析器，而在它支持的众多的解析器中，lxml 是性能最佳的，那么 BeautifulSoup 对象的各种方法可以理解为是对 lxml 的封装，换句话说，BeautifulSoup 本质上并没有创造出自己的解析方式，而是建立在各种解析器的基础上。考虑到其他一些内部耗时因素，BeautifulSoup 注定会比 lxml 甚至是任何一个构建对象时使用的解析器要慢，要更耗费空间。只有付出这样子的代价才能够换来它的简洁、优美与用户友好性。 

# Task3：安装selenium并学习和学习IP相关知识

**Q1: 怎么在ip被封之后实现自动更换代理池内的代理？**

A1: 用random.choice 随机选取ip

**Q2: 如何用一句通俗的语言解释清楚request、beautifulsoup和selenium三者与浏览器之间的关系？**

A2: BeautifulSoup：处理速度快，同时可以**连续**查找，主要**用于静态网页** 

经过BeautifulSoup处理以后，编码方式都变成了Unicode,需要将其变成所需的编码方式：可以利用encode(‘需要的编码’)，还可以利用 BeautifulSoup(网页/html, lxml/xml”).prettify(‘需要的编码’) 可以利用soup.original_encoding检测原来的编码。
Selenium：主要用于**动态网页**，查找速度慢，解析时要注意 .find_**elements**_by_xpath和.find_**element**_by_xpath有区别，同时利用浏览器时要配置。 .PhantomJS： 
drive=webdriver.PhantomJS(‘D:\Anaconda2\phantomjswindow**s\bin**phantomjs.exe’) 

**Q3: 构建好代理池后，如果在一次爬虫中自动切换代理？ 比如代理无效，或者代理ip被封，这时自动切换下一个ip。**

A3: 首先你要有一个ip代理池（如果比较豪可以自己买真实ip自行搭建，好处独享有技术门槛,或者找第三方ip代理商对接,好吃廉价,但ip不独享）， 真实ip需要自己写程序来代理转发，第三方ip代理商则会提供相关转发API,直接调用就可以，这东西没什么技术难度 

**Q4: ip_list.append(f'{protpcol}://{ip}:{port}')这里的f是格式化？**

A4:
1. 从代理*ip*网站爬取*IP*地址及端口号并储存
2. 验证*ip*是否能用
3. 格式化*ip*地址
4. 在*requests*中使用代理*ip*爬取网站

# Task4：实战大项目

**Q1: 图形验证码应该如何解决**

A1: 验证码问题，可以先截取出验证码在网页中的位置，然后保存为图片，使用ocr的包进行识别（没有做，只做到保存验证码的图片，当然要首先知道验证码位置）

**Q2: 验证码问题一般怎么实现？只有打码平台一种方法吗？这些图像码是自动生成的还是固定的图片？使用代理后是不是就不能使用Cookie方式登录了？**

A2: 解决方法有2中：
    一种是在请求的时候带上/webserver/前缀，比如把上文的 ../user/getCode.do改为  ../webserver/user/getCode.do；
    第二种是修改代理的实现，把82服务器上返回的http头上的set-cookie内的Path改为  Path=/，代码如下。
    在vue的webpack脚手架中的build/dev-server.js内的 * Object.keys(proxyTable).forEach(function (context) * 的实现改为如下形式
![图片](https://uploader.shimo.im/f/uG3dS1q0GPsiNRua.png!thumbnail)

**Q3: 还是图片验证码的问题没有解决2. xpath遇到回复拆分为了多个对象，使用posts[i].xpath('string()').strip()解决了，如果回复直接获取text()，有没有办法解决数据拆分的问题呢**

【换行符】 用字符的ascii码的值来判断，代表换行的 **回车**，**换行** 的ascii值分别为13,10
 用string的trim方法消除首尾多余空格,/\s+/匹配有效数据数据之间的空格做相应的处理

**Q4: selenium的find_element_by_name里面的name怎么找呢**
A4: ![图片](https://uploader.shimo.im/f/fyWMoodYNx4zZ7On.png!thumbnail)

**Q5: 爬虫反扒验证码解决途径？**

A5: 我能想到是1，使用打码平台；2，利用bat的云服务识别；3，自己破解js代码或者利用selenium模拟操作。
