
# 目录
**Task1**
Q1：使用pytorch对cnews数据搭建了一个textcnn模型，但是采用的是基于词的模型，没有尝试过基于字符的textcnn模型，对于这两种方法的优劣，就我的理解而言，基于词级别的textcnn需要对文本分词，而分词的效果非常依赖分词工具，虽然可以通过加入用户词典帮助分词，但是对于有些内容依然很难分词正确；基于字符级别的textcnn模型，不需要分词，但是会把所有字符都放到模型中，而对于文本数据而言，包含很多停用词，这样不加区分的都放入模型中，会加入很多噪声，影响模型的效果。不知道其他的小伙伴们对于词语级别的textcnn和字符级别的textcnn的优劣还有什么见解？
Q2：中英文因为语法、语义等不同，如果我们自行收集的数据中，我们在对中文和英文的预处理时分别需要注意哪些地方？
Q3：样本不均衡如何训练?
Q4:  文本分类预处理技术有哪些？
Q5:1、数据处理时是统计了每个字出现的次数，然后选取的前5000个，再对原句进行onehot，这样做的好处有哪些？ 2、词向量维度是否理解为每个字用一个64个数字表示，这里卷积核小于embedding的维度，这个有关系吗？图像的我可以理解，换文字的话，感觉embdding的维度才表示那个字，还是可以同理解为图像，这里我不是很明白，希望高手们给个解释

**Task2**
Q1:使用词向量的时候，有时候会使用多种词向量拼接，比如训练的word2vec向量100维，fasttext向量也是100维，那么我们横向拼接连起来可以得到200维的向量，纵向拼接起来求均值可以得到新的100维向量，而这种两种方法在理论上有没有什么依据或者可解释性
Q2: word2vec如何处理同义词？
Q3: Word2Vec 中使用的更为简单的 Negative Sampling 为什么能得到和 层次softmax 基本一样的效果？
Q4: word2vec的大致流程是怎样的，以及CBOW和Skip-Gram两者在实际中哪种用的比较多
Q5：我做了word2vec之外的两种词表示方法，一种是使用word context，另一种是node2vec，应用了context graph还是啥的文本表示方法。挺有趣的，大家可以看看，第一种方法可不可以用来做nlp下游任务？node2vec基本上和word2vec是一样的东西，只是文本被用以图的形式表示了，这样的嵌入方法能给嵌入结果带来什么样的影响？

**Task3**
Q1: 若是做预处理，比如分词，去停止词之类得，就会耗费挺多时间，但不做预处理又得不到好的结果，问，有没有可以迅速做完预处理得办法，尤其是中文文本
Q2: word2vec和fasttext的主要区别在哪里？2.优化器一般常用Adam和SGD，不过二者在使用上有什么区别，或者说哪些场景适合Adam，哪些场景适合SGD？
Q3: fastText有两种训练词向量的模式，一个监督训练，一个 无监督训练。监督训练里pretrainedVectors怎么用？主要是想要监督模型那个quatization模型压缩功能。另外fastText监督模型，在数据量很小的时候（几百句话，每条句子都很短）训练出的模型效果很差，有没有人碰到同样的情况，如何破?
Q4: 1.L1 正则项貌似不可导吧，具体实现过程中梯度下降是怎么求解的呢？为什么 L1 正则化能产生稀疏解？产生稀疏解的好处在哪里？
2.为什么 fastText 没有使用 Negative Sampling ？而是使用 层次 softMax？
3.神经网络模型和树模型相比，有哪些异同？有哪些优缺点？GBDT+LR 一般就能取得不错的效果了，那神经网络模型的优势在哪里？ 
Q5:Fasttext采用了Hash桶的方式，把所有的n-gram都哈希到buckets个桶中。
如果词数不是很多，可以把bucket设置的小一点，否则预留会预留太多bucket使模型太大。这里如何合理设置bucket的大小？
Q6: FastText如果将平均池化层转化为最大池化层，效果会更好吗？

**Task4**
Q1: textcnn和textrnn相比起来有没有优势之处？
Q2:问题1:textcnn和lstm网络比哪个更适合做分类任务。问题2:相对于情感分析这个稍微特殊的分类任务哪个网络会更好一点
Q3:卷积层过滤器的尺寸是一个过滤器输入尺寸的大小，深度指的是输出节点矩阵的深度。在具体的卷积过程中，从输入层到卷积层是不是可以理解为把多张图片在图片深度(channel)方向拼接然后卷积再到下一层？

**Task5**
Q1: 文本数据的长短存在一定比例的差别，若是按着长文本的标准来做向量表示，则需要填充成维度过高的向量，若是按着短文本来裁剪，长文本是否存在着丢失特征的情况，是否有办法来找到一个合理的取舍。
Q2: LSTM如何来避免梯度弥散和梯度爆炸？
Q3:有什么办法可以加快LSTM的训练速度？
Q4:CNN 也可以通过叠加的方式来获取长距离的信息，同时采取特定的方法来处理变长序列，那么 RNN 的优势体现在哪里？
Q5: 1. rnn使用自循环来学习位置信息，cnn使用dilation操作来间接学习位置信息，attention机制使用随机生成或正玄余玄编码位置信息，深度学习有没有其他帮助模型学习位置信息的操作？
2. 如果rnn可以看成一个全连接的完全图图模型，那么为什么这篇文章[https://arxiv.org/pdf/1904.11816v2.pdf中的模型中有明显的节点全连接操作](https://arxiv.org/pdf/1904.11816v2.pdf中的模型中有明显的节点全连接操作)，根据作者，会产生比单独使用rnn更好的效果。

**Task6**
Q1:在模型改进当中，需不需要考虑attention的调整？
Q2:如何可视化Attention？
Q3:attention机制以及self-attention机制主要在解决一个什么样的问题？看到现在NLP领域很多基于attention机制的网络结构出现，在不同任务上都取得于RNN/CNN更好的效果，为什么呢？
Q4:1，attention的模型，霸榜nlp各种任务。attention在自己源头任务图像识别里去没有那么受关注。是因为两个任务数据本质有一些区别？还是别的原因？2，有没有人了解过attention不太适合的nlp任务？
Q5:乘法attention和加法attention有啥区别？怎么体现的乘法，具体应用有讲究么？

**Task7**
Q1. 谷歌在训练Bert的时候输入字向量是不是可训练的状态（trainable）？如果是的话，是不是可以拿这些字向量来做下游任务。Bert网络太大，运行的时候需要提供大空间而且需要计算很耗费资源。所以一些对性能要求不高的任务，如果能拿bert的输入字向量，跳过bert网络的话会好一些。
Q2. 所以如果bert字向量可以拿来像word2vec一样使用的话，经过这么大的网络和语料训练出来的字向量是不是比fastText或其它word2vec的向量好一些？
Q3. BERT在利用句向量进行文本分类时的优缺点
Q4. bert与transform的关系是什么？
Q5. BERT和XLnet比，最大的优势是什么？

# Task1：
**Q2: ****中英文因为语法、语义等不同，如果我们自行收集的数据中，我们在对中文和英文的预处理时分别需要注意哪些地方？**
A2: 从目前各方面的实验结果来看，是的，使得不同类型的任务都获得的了一个较大的提升；详见：[https://zhuanlan.zhihu.com/p/68446772](https://zhuanlan.zhihu.com/p/68446772)

**Q3: ****样本不均衡如何训练?**
A3: 解决样本不均衡的方法有两类：
在数据层面，修改各类别的分布；
在分类器层面，对算法或目标函数进行改进。
（将上述两类进行融合也是解决的一种方法）
1. 选择一些可以能够显著划分的特征进行训练。
1. 如果数据量比较大，可以移除部分类别中数据量多的数据，达到正态分布或者平衡的状态。
1. 对数据加上权重，为数量少的样本加上较大权重，为数量多的样本加上较小权重

**Q4: ****文本分类预处理技术有哪些？**
A4: 分词，统计词出现次数，加入<start>等需要的字符，建立 （词：数字）的词表，把句子转化成数字，词嵌入

**Q5: 1、数据处理时是统计了每个字出现的次数，然后选取的前5000个，再对原句进行onehot，这样做的好处有哪些？ **
A5: 1、数据处理时是统计了每个字出现的次数，然后选取的前5000个，再对原句进行onehot，这样做的好处有哪些？ 选区前5000个可以过滤一些较少出现的词汇，训练的时候快些把，把词转成one-hot的形式如果词表很大的话，假设词表5000，每个词都用5000维表示，太吃内存，所以用one-hot表示词的时候，使用出现频率较高的词汇（5000），但one-hot表示法不能让词与词间有关联，wordvec代替one-hot表示词，不仅可以降维，还可以让词与词有关联

# Task2：
**Q4: **** word2vec的大致流程是怎样的，以及CBOW和Skip-Gram两者在实际中哪种用的比较多**
A4: 我在看资料时，有看到这样一句话:CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好(来自：[https://blog.csdn.net/mylove0414/article/details/61616617)](https://blog.csdn.net/mylove0414/article/details/61616617))
大致可以分为：准备好语料并预处理，统计词频，构造树形结构，初始化词向量以及非叶节点的向量，训练

# Task3：
**Q1: ****若是做预处理，比如分词，去停止词之类得，就会耗费挺多时间，但不做预处理又得不到好的结果，问，有没有可以迅速做完预处理得办法，尤其是中文文本**
A1: fasttext增加了subwords特性。subwords其实就是一个词的character-level的n-gram。那么中文是否可以将汉字拆分为偏旁和部首，这样做，可以做字的相似性计算。

# Tassk4：
**Q2: 问题1:textcnn和lstm网络比哪个更适合做分类任务。问题2:相对于情感分析这个稍微特殊的分类任务哪个网络会更好一点**
A2: TextCNN提取类似于n-gram的特征。忽略了词序，所以在词序不敏感的场景效果很好，一般CNN是一个很强的baseline，LSTM可以捕捉到序列信息，在情感分析这种词序很重要的应用场景中效果更好
情感分析，我认为在CNN效果较好，因为文本中有消极或者积极的词，都能代表情感。但是这效果会随文本变长而变差，因为长距离依赖。

**CNN理解：**
CNN的核心点在于可以捕获信息的局部相关性，具体到文本分类任务中可以利用CNN来提取句子中类似N-Gram的关键信息。TextCNN擅长捕获更短的序列信息，但是TextRNN擅长捕获更长的序列信息。具体到文本分类任务中，BiLSTM从某种意义上可以理解为可以捕获变长且双向的N-Gram信息。将CNN和RNN用在文本分类中都能取得显著的效果，但是有一个不错的地方就是可解释性不好，特别是去分析错误案例的时候，而注意力机制[Attention]能够很好的给出每个词对结果的贡献程度，已经成为Seq2Seq模型的标配，实际上文本分类也可以理解为一种特殊的Seq2Seq模型。因此，注意力机制的引入，可以在某种程度上提高深度学习文本分类模型的可解释性。

# Task5：
**Q2: ****LSTM如何来避免梯度弥散和梯度爆炸？**
A2: LSTM的三个门，遗忘门，输入门，输出门，这三个门的激活函数是sigmoid，这也就意味着这三个门的输出要么接近于0 ， 要么接近于1，当门为1时， 梯度能够很好的在LSTM中传递，很大程度上减轻了梯度消失发生的概率， 当门为0时，说明上一时刻的信息对当前时刻没有影响， 我们也就没有必要传递梯度回去来更新参数了。所以， 这就是为什么通过门机制就能够解决梯度的原因。

# Task7：
**Q1: ****谷歌在训练Bert的时候输入字向量是不是可训练的状态（trainable）？如果是的话，是不是可以拿这些字向量来做下游任务。Bert网络太大，运行的时候需要提供大空间而且需要计算很耗费资源。所以一些对性能要求不高的任务，如果能拿bert的输入字向量，跳过bert网络的话会好一些。**
A1: 我没理解错的话，BERT的目的就算为了与下游任务结合；
“BERT模型的目标是利用大规模无标注语料训练、获得文本的包含丰富语义信息的Representation，即：文本的语义表示，然后将文本的语义表示在特定NLP任务中作微调，最终应用于该NLP任务” 
见：[https://mp.weixin.qq.com/s/HOt11jG0DhhVuFWYhKNXtg](https://mp.weixin.qq.com/s/HOt11jG0DhhVuFWYhKNXtg)

**Q4: ****bert与transform的关系是什么？**
A4: 我理解的是bert是借用了transformer中的transformer Encoder 这个元件，把这个当作一个普通的神经网络的神经元进行建模；就像我们说的LSTM单元中的LSTMcell;
最直接的是这样建模可以达到很好的效果；
更深层次的是，每个transformer Encoder都叫较强的学习能力，相比于其他单元；其内部的多头 Self-Attention可以从预训练任务中学到更多有用的东西。
