NLP进阶小测试 draft
Task1
* Q1：使用pytorch对cnews数据搭建了一个textcnn模型，但是采用的是基于词的模型，没有尝试过基于字符的textcnn模型，对于这两种方法的优劣，就我的理解而言，基于词级别的textcnn需要对文本分词，而分词的效果非常依赖分词工具，虽然可以通过加入用户词典帮助分词，但是对于有些内容依然很难分词正确；基于字符级别的textcnn模型，不需要分词，但是会把所有字符都放到模型中，而对于文本数据而言，包含很多停用词，这样不加区分的都放入模型中，会加入很多噪声，影响模型的效果。不知道其他的小伙伴们对于词语级别的textcnn和字符级别的textcnn的优劣还有什么见解？
* Q2：中英文因为语法、语义等不同，如果我们自行收集的数据中，我们在对中文和英文的预处理时分别需要注意哪些地方？
* Q3：样本不均衡如何训练?
* Q4: 文本分类预处理技术有哪些？
* Q5:1、数据处理时是统计了每个字出现的次数，然后选取的前5000个，再对原句进行onehot，这样做的好处有哪些？ 2、词向量维度是否理解为每个字用一个64个数字表示，这里卷积核小于embedding的维度，这个有关系吗？图像的我可以理解，换文字的话，感觉embdding的维度才表示那个字，还是可以同理解为图像，这里我不是很明白，希望高手们给个解释
 
Task2
* Q1:使用词向量的时候，有时候会使用多种词向量拼接，比如训练的word2vec向量100维，fasttext向量也是100维，那么我们横向拼接连起来可以得到200维的向量，纵向拼接起来求均值可以得到新的100维向量，而这种两种方法在理论上有没有什么依据或者可解释性
* Q2: Word2Vec 中使用的更为简单的 Negative Sampling 为什么能得到和 层次softmax 基本一样的效果？
* Q3: word2vec的大致流程是怎样的，以及CBOW和Skip-Gram两者在实际中哪种用的比较多
* Q4：我做了word2vec之外的两种词表示方法，一种是使用word context，另一种是node2vec，应用了context graph还是啥的文本表示方法。挺有趣的，大家可以看看，第一种方法可不可以用来做nlp下游任务？node2vec基本上和word2vec是一样的东西，只是文本被用以图的形式表示了，这样的嵌入方法能给嵌入结果带来什么样的影响？
 
Task3
* Q1: 若是做预处理，比如分词，去停止词之类得，就会耗费挺多时间，但不做预处理又得不到好的结果，问，有没有可以迅速做完预处理得办法，尤其是中文文本
* Q2: word2vec和fasttext的主要区别在哪里？2.优化器一般常用Adam和SGD，不过二者在使用上有什么区别，或者说哪些场景适合Adam，哪些场景适合SGD？
* Q3: fastText有两种训练词向量的模式，一个监督训练，一个 无监督训练。监督训练里pretrainedVectors怎么用？主要是想要监督模型那个quatization模型压缩功能。另外fastText监督模型，在数据量很小的时候（几百句话，每条句子都很短）训练出的模型效果很差，有没有人碰到同样的情况，如何破?
* Q4: 1.L1 正则项貌似不可导吧，具体实现过程中梯度下降是怎么求解的呢？为什么 L1 正则化能产生稀疏解？产生稀疏解的好处在哪里？
* 2.为什么 fastText 没有使用 Negative Sampling ？而是使用 层次 softMax？
* 3.神经网络模型和树模型相比，有哪些异同？有哪些优缺点？GBDT+LR 一般就能取得不错的效果了，那神经网络模型的优势在哪里？ 
* Q5:Fasttext采用了Hash桶的方式，把所有的n-gram都哈希到buckets个桶中。
如果词数不是很多，可以把bucket设置的小一点，否则预留会预留太多bucket使模型太大。这里如何合理设置bucket的大小？
* Q6: FastText如果将平均池化层转化为最大池化层，效果会更好吗？
 
Task4
	Q1: textcnn和textrnn相比起来有没有优势之处？
	Q2:问题1:textcnn和lstm网络比哪个更适合做分类任务。问题2:相对于情感分析这个稍微特殊的分类任务哪个网络会更好一点
	Q3:卷积层过滤器的尺寸是一个过滤器输入尺寸的大小，深度指的是输出节点矩阵的深度。在具体的卷积过程中，从输入层到卷积层是不是可以理解为把多张图片在图片深度(channel)方向拼接然后卷积再到下一层？
 
Task5
* Q1: 文本数据的长短存在一定比例的差别，若是按着长文本的标准来做向量表示，则需要填充成维度过高的向量，若是按着短文本来裁剪，长文本是否存在着丢失特征的情况，是否有办法来找到一个合理的取舍。
* Q2: LSTM如何来避免梯度弥散和梯度爆炸？
* Q3:有什么办法可以加快LSTM的训练速度？
* Q4:CNN 也可以通过叠加的方式来获取长距离的信息，同时采取特定的方法来处理变长序列，那么 RNN 的优势体现在哪里？
* Q5:rnn使用自循环来学习位置信息，cnn使用dilation操作来间接学习位置信息，attention机制使用随机生成或正玄余玄编码位置信息，深度学习有没有其他帮助模型学习位置信息的操作？
* Q6:如果rnn可以看成一个全连接的完全图图模型，那么为什么这篇文章https://arxiv.org/pdf/1904.11816v2.pdf中的模型中有明显的节点全连接操作，根据作者，会产生比单独使用rnn更好的效果。
 
Task6
* Q1:在模型改进当中，需不需要考虑attention的调整？
* Q2:如何可视化Attention？
* Q3:attention机制以及self-attention机制主要在解决一个什么样的问题？看到现在NLP领域很多基于attention机制的网络结构出现，在不同任务上都取得于RNN/CNN更好的效果，为什么呢？
* Q4:1，attention的模型，霸榜nlp各种任务。attention在自己源头任务图像识别里去没有那么受关注。是因为两个任务数据本质有一些区别？还是别的原因？2，有没有人了解过attention不太适合的nlp任务？
* Q5:乘法attention和加法attention有啥区别？怎么体现的乘法，具体应用有讲究么？
 
Task7
* Q1. 谷歌在训练Bert的时候输入字向量是不是可训练的状态（trainable）？如果是的话，是不是可以拿这些字向量来做下游任务。Bert网络太大，运行的时候需要提供大空间而且需要计算很耗费资源。所以一些对性能要求不高的任务，如果能拿bert的输入字向量，跳过bert网络的话会好一些。
* Q2. 所以如果bert字向量可以拿来像word2vec一样使用的话，经过这么大的网络和语料训练出来的字向量是不是比fastText或其它word2vec的向量好一些？
* Q3. BERT在利用句向量进行文本分类时的优缺点
* Q4. bert与transform的关系是什么？
* Q5. BERT和XLnet比，最大的优势是什么？
